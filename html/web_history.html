<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The World Wide Web: A Comprehensive History and Future Trends</title>
  </head>
  <body>
    <header>
      <h1>The World Wide Web: A Comprehensive History and Future Trends</h1>
    </header>

    <main>
      <section>
        <h2>Precursors to the Web</h2>
        <ul>
          <li>
            <h3>1945: Vannevar Bush's "Memex"</h3>
            <p>
              Memex [memory expansion] is a hypothetical electromechanical
              device for interacting with microform documents and described in
              Vannevar Bush's 1945 article
              <a href="https://en.wikipedia.org/wiki/As_We_May_Think"
                >"As We May Think" </a
              >. Bush envisioned the memex as a device in which individuals
              would compress and store all of their books, records, and
              communications, "mechanized so that it may be consulted with
              exceeding speed and flexibility". The individual was supposed to
              use the memex as an automatic personal filing system, making the
              memex "an enlarged intimate supplement to his memory". The concept
              of the memex influenced the development of early hypertext
              systems, eventually leading to the creation of the World Wide Web,
              and personal knowledge base software. The hypothetical
              implementation depicted by Bush for the purpose of concrete
              illustration was based upon a document bookmark list of static
              microfilm pages and lacked a true hypertext system, where parts of
              pages would have internal structure beyond the common textual
              format.
            </p>
          </li>
          <li>
            <h3>1960s: ARPANET</h3>
            <p>
              ARPANET, created by the Advanced Research Projects Agency in 1969,
              was an early computer network that used packet-switching. It was
              designed to be a secure communication medium for sensitive
              military data and research collaboration among leading US
              institutions. ARPANET initially ran on Network Control Protocol
              (NCP) and later transitioned to TCP/IP, making it a crucial part
              of the emerging Internet. The network's robust design included
              multiple nodes and telecommunication links, ensuring reliability
              and survivability. ARPANET's significant role in Internet
              development is evident in its adoption of TCP/IP, which became the
              standard protocol for the Internet.
            </p>
          </li>
          <li>
            <h3>1968: Doug Engelbart's "Mother of All Demos"</h3>
            <p>
              The "Mother of All Demos" in 1968 by Douglas Engelbart
              significantly contributed to the development of the World Wide Web
              through the introduction of several groundbreaking concepts. These
              key elements included windows, hypertext, graphics, efficient
              navigation and command input, video conferencing, the computer
              mouse, word processing, dynamic file linking, revision control,
              and a collaborative real-time editor. Engelbart's vision for a
              Memex machine that could augment human intelligence laid the
              foundation for modern personal computing.
            </p>
          </li>
          <li>
            <h3>1970s: TCP/IP Development</h3>
            <p>
              Initially referred to as the DOD Internet Architecture Model, the
              Internet protocol suite has its roots in research and development
              sponsored by the Defense Advanced Research Projects Agency (DARPA)
              in the late 1960s. TCP/IP, the Transmission Control
              Protocol/Internet Protocol suite, was designed in the 1970s by two
              DARPA scientists, Vint Cerf and Bob Kahn, often referred to as the
              “fathers of the Internet.” Initially, the TCP/IP model was
              developed for the ARPANET, the precursor to the Internet.
            </p>
          </li>
        </ul>
      </section>

      <section>
        <h2>The Birth of the World Wide Web</h2>
        <ul>
          <li>
            <h3>CERN</h3>
            <p>
              While working at CERN, Tim Berners-Lee became frustrated with the
              inefficiencies and difficulties posed by finding information
              stored on different computers. On 12 March 1989, he submitted a
              memorandum, titled "Information Management: A Proposal", to the
              management at CERN. The proposal used the term "web" and was based
              on "a large hypertext database with typed links". It described a
              system called "Mesh" that referenced ENQUIRE, the database and
              software project he had built in 1980, with a more elaborate
              information management system based on links embedded as text:
              "Imagine, then, the references in this document all being
              associated with the network address of the thing to which they
              referred, so that while reading this document, you could skip to
              them with a click of the mouse." Such a system, he explained,
              could be referred to using one of the existing meanings of the
              word hypertext, a term that he says was coined in the 1950s.
              Berners-Lee notes the possibility of multimedia documents that
              include graphics, speech and video, which he terms hypermedia.
            </p>
            <p>
              Although the proposal attracted little interest, Berners-Lee was
              encouraged by his manager, Mike Sendall, to begin implementing his
              system on a newly acquired NeXT workstation. He considered several
              names, including Information Mesh, The Information Mine or Mine of
              Information, but settled on World Wide Web. Berners-Lee found an
              enthusiastic supporter in his colleague and fellow hypertext
              enthusiast Robert Cailliau who began to promote the proposed
              system throughout CERN. Berners-Lee and Cailliau pitched
              Berners-Lee's ideas to the European Conference on Hypertext
              Technology in September 1990, but found no vendors who could
              appreciate his vision.
            </p>
            <p>
              Berners-Lee's breakthrough was to marry hypertext to the Internet.
              In his book Weaving The Web, he explains that he had repeatedly
              suggested to members of both technical communities that a marriage
              between the two technologies was possible. But, when no one took
              up his invitation, he finally assumed the project himself. In the
              process, he developed three essential technologies:
            </p>

            <ul>
              <li>
                a system of globally unique identifiers for resources on the Web
                and elsewhere, the universal document identifier (UDI), later
                known as uniform resource locator (URL);
              </li>
              <li>the publishing language Hypertext Markup Language (HTML);</li>
              <li>the Hypertext Transfer Protocol (HTTP).</li>
            </ul>
            <p>
              With help from Cailliau he published a more formal proposal on 12
              November 1990 to build a "hypertext project" called World Wide Web
              (abbreviated "W3") as a "web" of "hypertext documents" to be
              viewed by "browsers" using a client–server architecture. The
              proposal was modelled after the Standard Generalized Markup
              Language (SGML) reader Dynatext by Electronic Book Technology, a
              spin-off from the Institute for Research in Information and
              Scholarship at Brown University. The Dynatext system, licensed by
              CERN, was considered too expensive and had an inappropriate
              licensing policy for use in the general high energy physics
              community, namely a fee for each document and each document
              alteration.[citation needed]
            </p>
            <p>
              At this point HTML and HTTP had already been in development for
              about two months and the first web server was about a month from
              completing its first successful test. Berners-Lee's proposal
              estimated that a read-only Web would be developed within three
              months and that it would take six months to achieve "the creation
              of new links and new material by readers, [so that] authorship
              becomes universal" as well as "the automatic notification of a
              reader when new material of interest to him/her has become
              available".
            </p>
            <p>
              By December 1990, Berners-Lee and his work team had built all the
              tools necessary for a working Web: the HyperText Transfer Protocol
              (HTTP), the HyperText Markup Language (HTML), the first web
              browser (named WorldWideWeb, which was also a web editor), the
              first web server (later known as CERN httpd) and the first web
              site (http://info.cern.ch) containing the first web pages that
              described the project itself was published on 20 December 1990.
              The browser could access Usenet newsgroups and FTP files as well.
              A NeXT Computer was used by Berners-Lee as the web server and also
              to write the web browser. Working with Berners-Lee at CERN, Nicola
              Pellow developed the first cross-platform web browser, the Line
              Mode Browser.
            </p>
          </li>
          <li>
            <h3>1989: Tim Berners-Lee's Proposal</h3>
            <p>
              In March 1989, Tim laid out his vision for what would become the
              web in a document called
              <a href="https://cds.cern.ch/record/369245/files/dd-89-001.pdf"
                >“Information Management: A Proposal”</a
              >. Believe it or not, Tim’s initial proposal was not immediately
              accepted. In fact, his boss at the time, Mike Sendall, noted the
              words “Vague but exciting” on the cover. The web was never an
              official CERN project, but Mike managed to give Tim time to work
              on it in September 1990. He began work using a NeXT computer, one
              of Steve Jobs’ early products.
            </p>
          </li>
          <li>
            <h3>1990: Development of Core Technologies</h3>
            <p>
              By October of 1990, Tim had written the three fundamental
              technologies that remain the foundation of today's web (and which
              you may have seen appear on parts of your web browser):
            </p>
            <ol>
              <li>
                HTML: HyperText Markup Language. The markup (formatting)
                language for the web.
              </li>
              <li>
                URI: Uniform Resource Identifier. A kind of “address” that is
                unique and used to identify to each resource on the web. It is
                also commonly called a URL.
              </li>
              <li>
                HTTP: Hypertext Transfer Protocol. Allows for the retrieval of
                linked resources from across the web.
              </li>
            </ol>
          </li>
          <li>
            <h3>1991: The Web Goes Public</h3>
            <p>
              At this stage, there were essentially only two kinds of browser.
              One was the original development version, which was sophisticated
              but available only on NeXT machines. The other was the ‘line-mode’
              browser, which was easy to install and run on any platform but
              limited in power and user-friendliness.
              <br />
              Tim Berners-Lee had the first Web server and browser up and
              running at CERN, demonstrating his ideas. He developed the code
              for his Web server on a NeXT computer.
              <a href="http://info.cern.ch/">First web</a>
            </p>
          </li>
        </ul>
      </section>

      <section>
        <h2>Early Growth and Development</h2>
        <ul>
          <li>
            <h3>From Gopher to the WWW</h3>
            <p>
              In the early 1990s, Internet-based projects such as Archie,
              Gopher, Wide Area Information Servers (WAIS), and the FTP Archive
              list attempted to create ways to organize distributed data. Gopher
              was a document browsing system for the Internet, released in 1991
              by the University of Minnesota. Invented by Mark P. McCahill, it
              became the first commonly used hypertext interface to the
              Internet. While Gopher menu items were examples of hypertext, they
              were not commonly perceived in that way[clarification needed]. In
              less than a year, there were hundreds of Gopher servers. It
              offered a viable alternative to the World Wide Web in the early
              1990s and the consensus was that Gopher would be the primary way
              that people would interact with the Internet. However, in 1993,
              the University of Minnesota declared that Gopher was proprietary
              and would have to be licensed. In response, on 30 April 1993, CERN
              announced that the World Wide Web would be free to anyone, with no
              fees due, and released their code into the public domain. This
              made it possible to develop servers and clients independently and
              to add extensions without licensing restrictions.[citation needed]
              Coming two months after the announcement that the server
              implementation of the Gopher protocol was no longer free to use,
              this spurred the development of various browsers which
              precipitated a rapid shift away from Gopher. By releasing
              Berners-Lee's invention for public use, CERN encouraged and
              enabled its widespread use. Early websites intermingled links for
              both the HTTP web protocol and the Gopher protocol, which provided
              access to content through hypertext menus presented as a file
              system rather than through HTML files. Early Web users would
              navigate either by bookmarking popular directory pages or by
              consulting updated lists such as the NCSA "What's New" page. Some
              sites were also indexed by WAIS, enabling users to submit
              full-text searches similar to the capability later provided by
              search engines
            </p>
          </li>
          <li>
            <h3>1992: First Graphical Browser</h3>
            <p>
              Pei-Yuan Wei develops ViolaWWW, one of the first graphical
              browsers.
              <br />
              <a
                href="https://line-mode.cern.ch/www/hypertext/WWW/TheProject.html"
                >First Web Simulator</a
              >
            </p>
          </li>
          <li>
            <h3>1993: Mosaic and the Web's Popularity Surge</h3>
            <p>
              The National Center for Supercomputing Applications (NCSA) at the
              University of Illinois at Urbana–Champaign (UIUC) established a
              website in November 1992. After Marc Andreessen, a student at
              UIUC, was shown ViolaWWW in late 1992, he began work on Mosaic
              with another UIUC student Eric Bina, using funding from the
              High-Performance Computing and Communications Initiative, a
              US-federal research and development program initiated by US
              Senator Al Gore. Andreessen and Bina released a Unix version of
              the browser in February 1993; Mac and Windows versions followed in
              August 1993. The browser gained popularity due to its strong
              support of integrated multimedia, and the authors' rapid response
              to user bug reports and recommendations for new features.
              Historians generally agree that the 1993 introduction of the
              Mosaic web browser was a turning point for the World Wide Web.
            </p>
          </li>
          <li>
            <h3>Early Growth</h3>
            <p>
              In keeping with its origins at CERN, early adopters of the Web
              were primarily university-based scientific departments or physics
              laboratories such as SLAC and Fermilab. By January 1993 there were
              fifty web servers across the world. By October 1993 there were
              over five hundred servers online, including some notable
              websites.Practical media distribution and streaming media over the
              Web was made possible by advances in data compression, due to the
              impractically high bandwidth requirements of uncompressed media.
              Following the introduction of the Web, several media formats based
              on discrete cosine transform (DCT) were introduced for practical
              media distribution and streaming over the Web, including the MPEG
              video format in 1991 and the JPEG image format in 1992. The high
              level of image compression made JPEG a good format for
              compensating slow Internet access speeds, typical in the age of
              dial-up Internet access. JPEG became the most widely used image
              format for the World Wide Web. A DCT variation, the modified
              discrete cosine transform (MDCT) algorithm, led to the development
              of MP3, which was introduced in 1991 and became the first popular
              audio format on the Web. In 1992 the Computing and Networking
              Department of CERN, headed by David Williams, withdrew support of
              Berners-Lee's work. A two-page email sent by Williams stated that
              the work of Berners-Lee, with the goal of creating a facility to
              exchange information such as results and comments from CERN
              experiments to the scientific community, was not the core activity
              of CERN and was a misallocation of CERN's IT resources. Following
              this decision, Tim Berners-Lee left CERN for the Massachusetts
              Institute of Technology (MIT), where he continued to develop HTTP.
            </p>
          </li>
        </ul>
      </section>

      <section>
        <h2>The Web Goes Mainstream</h2>
        <ul>
          <li>
            <h3>World Wide Web Consortium</h3>
            <p>
              The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee
              after he left the European Organization for Nuclear Research
              (CERN) in September/October 1994 in order to create open standards
              for the Web. It was founded at the Massachusetts Institute of
              Technology Laboratory for Computer Science (MIT/LCS) with support
              from the Defense Advanced Research Projects Agency (DARPA), which
              had pioneered the Internet. A year later, a second site was
              founded at INRIA (a French national computer research lab) with
              support from the European Commission; and in 1996, a third
              continental site was created in Japan at Keio University. W3C
              comprised various companies that were willing to create standards
              and recommendations to improve the quality of the Web. Berners-Lee
              made the Web available freely, with no patent and no royalties
              due. The W3C decided that its standards must be based on
              royalty-free technology, so they can be easily adopted by anyone.
              Netscape and Microsoft, in the middle of a browser war, ignored
              the W3C and added elements to HTML ad hoc (e.g., blink and
              marquee). Finally, in 1995, Netscape and Microsoft came to their
              senses and agreed to abide by the W3C's standard.The W3C published
              the standard for HTML 4 in 1997, which included Cascading Style
              Sheets (CSS), giving designers more control over the appearance of
              web pages without the need for additional HTML tags. The W3C could
              not enforce compliance so none of the browsers were fully
              compliant. This frustrated web designers who formed the Web
              Standards Project (WaSP) in 1998 with the goal of cajoling
              compliance with standards. A List Apart and CSS Zen Garden were
              influential websites that promoted good design and adherence to
              standards. Nevertheless, AOL halted development of Netscape and
              Microsoft was slow to update IE. Mozilla and Apple both released
              browsers that aimed to be more standards compliant (Firefox and
              Safari), but were unable to dislodge IE as the dominant browser.
            </p>
          </li>
          <li>
            <h3>Commercialization, dot-com boom and bust, aftermath</h3>
            <p>
              As the Web grew in the mid-1990s, web directories and primitive
              search engines were created to index pages and allow people to
              find things. Commercial use restrictions on the Internet were
              lifted in 1995 when NSFNET was shut down. In the US, the online
              service America Online (AOL) offered their users a connection to
              the Internet via their own internal browser, using a dial-up
              Internet connection. In January 1994, Yahoo! was founded by Jerry
              Yang and David Filo, then students at Stanford University. Yahoo!
              Directory became the first popular web directory. Yahoo! Search,
              launched the same year, was the first popular search engine on the
              World Wide Web. Yahoo! became the quintessential example of a
              first mover on the Web. Online shopping began to emerge with the
              launch of Amazon's shopping site by Jeff Bezos in 1995 and eBay by
              Pierre Omidyar the same year. Netscape had a very successful IPO
              valuing the company at $2.9 billion despite the lack of profits
              and triggering the dot-com bubble. Increasing familiarity with the
              Web led to the growth of direct Web-based commerce (e-commerce)
              and instantaneous group communications worldwide. Many dot-com
              companies, displaying products on hypertext webpages, were added
              into the Web. Over the next 5 years, over a trillion dollars was
              raised to fund thousands of startups consisting of little more
              than a website. During the dot-com boom, many companies vied to
              create a dominant web portal in the belief that such a website
              would best be able to attract a large audience that in turn would
              attract online advertising revenue. While most of these portals
              offered a search engine, they were not interested in encouraging
              users to find other websites and leave the portal and instead
              concentrated on "sticky" content. In contrast, Google was a
              stripped-down search engine that delivered superior results. It
              was a hit with users who switched from portals to Google.
              Furthermore, with AdWords, Google had an effective business model.
              With the bursting of the dot-com bubble, many web portals either
              scaled back operations, floundered, or shut down entirely. AOL
              disbanded Netscape in 2003
            </p>
          </li>

          <li>
            <h4>Dot-com Bubble</h4>
            <p>
              In 2000, the dot-com bubble burst, and many dot-com startups went
              out of business after burning through their venture capital and
              failing to become profitable. However, many others, particularly
              online retailers like eBay and Amazon, blossomed and became highly
              profitable. More conventional retailers found online merchandising
              to be a profitable additional source of revenue. While some online
              entertainment and news outlets failed when their seed capital ran
              out, others persisted and eventually became economically
              self-sufficient. Traditional media outlets (newspaper publishers,
              broadcasters and cablecasters in particular) also found the Web to
              be a useful and profitable additional channel for content
              distribution, and an additional means to generate advertising
              revenue. The sites that survived and eventually prospered after
              the bubble burst had two things in common: a sound business plan,
              and a niche in the marketplace that was, if not unique,
              particularly well-defined and well-served.[citation needed]
            </p>

            <img
              src="https://upload.wikimedia.org/wikipedia/commons/8/84/Nasdaq_Composite_dot-com_bubble.svg"
              alt="The NASDAQ Composite index spiked in 2000 and then fell sharply as a result of the dot-com bubble."
              style="height: 150px; width: 300px"
            />
            <img
              src="https://upload.wikimedia.org/wikipedia/commons/5/50/US_VC_funding.png"
              alt="Quarterly U.S. venture capital investments, 1995–2017"
              style="height: 150px; width: 300px"
            />
            <p>
              In the aftermath of the dot-com bubble, telecommunications
              companies had a great deal of overcapacity as many Internet
              business clients went bust. That, plus ongoing investment in local
              cell infrastructure kept connectivity charges low, and helped to
              make high-speed Internet connectivity more affordable.[citation
              needed] During this time, a handful of companies found success
              developing business models that helped make the World Wide Web a
              more compelling experience. These include airline booking sites,
              Google's search engine and its profitable approach to
              keyword-based advertising, as well as eBay's auction site and
              Amazon.com's online department store. The low price of reaching
              millions worldwide, and the possibility of selling to or hearing
              from those people at the same moment when they were reached,
              promised to overturn established business dogma in advertising,
              mail-order sales, customer relationship management, and many more
              areas. The web was a new killer app—it could bring together
              unrelated buyers and sellers in seamless and low-cost ways.
              Entrepreneurs around the world developed new business models, and
              ran to their nearest venture capitalist. While some of the new
              entrepreneurs had experience in business and economics, the
              majority were simply people with ideas, and did not manage the
              capital influx prudently. Additionally, many dot-com business
              plans were predicated on the assumption that by using the
              Internet, they would bypass the distribution channels of existing
              businesses and therefore not have to compete with them; when the
              established businesses with strong existing brands developed their
              own Internet presence, these hopes were shattered, and the
              newcomers were left attempting to break into markets dominated by
              larger, more established businesses
            </p>
          </li>
          <li>
            <h3>Browser War</h3>
            <p>
              After graduating from UIUC, Andreessen and Jim Clark, former CEO
              of Silicon Graphics, met and formed Mosaic Communications
              Corporation in April 1994 to develop the Mosaic Netscape browser
              commercially. The company later changed its name to Netscape, and
              the browser was developed further as Netscape Navigator, which
              soon became the dominant web client. They also released the
              Netsite Commerce web server which could handle SSL requests, thus
              enabling e-commerce on the Web. SSL became the standard method to
              encrypt web traffic. Navigator 1.0 also introduced cookies, but
              Netscape did not publicize this feature. Netscape followed up with
              Navigator 2 in 1995 introducing frames, Java applets and
              JavaScript. In 1998, Netscape made Navigator open source and
              launched Mozilla. Microsoft licensed Mosaic from Spyglass and
              released Internet Explorer 1.0 that year and IE2 later the same
              year. IE2 added features pioneered at Netscape such as cookies,
              SSL, and JavaScript. The browser wars became a competition for
              dominance when Explorer was bundled with Windows. This led to the
              United States v. Microsoft Corporation antitrust lawsuit. IE3,
              released in 1996, added support for Java applets, ActiveX, and
              CSS. At this point, Microsoft began bundling IE with Windows. IE3
              managed to increase Microsoft's share of the browser market from
              under 10% to over 20%. IE4, released the following year,
              introduced Dynamic HTML setting the stage for the Web 2.0
              revolution. By 1998, IE was able to capture the majority of the
              desktop browser market. It would be the dominant browser for the
              next fourteen years. Google released their Chrome browser in 2008
              with the first JIT JavaScript engine, V8. Chrome overtook IE to
              become the dominant desktop browser in four years, and overtook
              Safari to become the dominant mobile browser in two. At the same
              time, Google open sourced Chrome's codebase as Chromium. Ryan Dahl
              used Chromium's V8 engine in 2009 to power an event driven runtime
              system, Node.js, which allowed JavaScript code to be used on
              servers as well as browsers. This led to the development of new
              software stacks such as MEAN. Thanks to frameworks such as
              Electron, developers can bundle up node applications as standalone
              desktop applications such as Slack. Acer and Samsung began selling
              Chromebooks, cheap laptops running ChromeOS capable of running web
              apps, in 2011. Over the next decade, more companies offered
              Chromebooks. Chromebooks outsold MacOS devices in 2020 to become
              the second most popular OS in the world. Other notable web
              browsers emerged including Mozilla's Firefox, Opera's Opera
              browser and Apple Safari.
            </p>
          </li>

          <li>
            <h3>Web 1.0</h3>
            <p>
              Web 1.0 is a retronym referring to the first stage of the World
              Wide Web's evolution, from roughly 1989 to 2004. According to
              Graham Cormode and Balachander Krishnamurthy, "content creators
              were few in Web 1.0 with the vast majority of users simply acting
              as consumers of content". Personal web pages were common,
              consisting mainly of static pages hosted on ISP-run web servers,
              or on free web hosting services such as Tripod and the now-defunct
              GeoCities. Some common design elements of a Web 1.0 site include:
              Static pages rather than dynamic HTML. Content provided from the
              server's filesystem rather than a relational database management
              system (RDBMS). Pages built using Server Side Includes or Common
              Gateway Interface (CGI) instead of a web application written in a
              dynamic programming language such as Perl, PHP, Python or
              Ruby.[clarification needed] The use of HTML 3.2-era elements such
              as frames and tables to position and align elements on a page.
              These were often used in combination with spacer GIFs.[citation
              needed] Proprietary HTML extensions, such as the &lt blink &gt and
              &lt marquee &gt tags, introduced during the first browser war.
              Online guestbooks. GIF buttons, graphics (typically 88×31 pixels
              in size) promoting web browsers, operating systems, text editors
              and various other products. HTML forms sent via email. Support for
              server side scripting was rare on shared servers during this
              period. To provide a feedback mechanism for web site visitors,
              mailto forms were used. A user would fill in a form, and upon
              clicking the form's submit button, their email client would launch
              and attempt to send an email containing the form's details. The
              popularity and complications of the mailto protocol led browser
              developers to incorporate email clients into their browsers
            </p>
          </li>
        </ul>
      </section>
      <section>
        <h2>2004–present: The Web as platform</h2>
        <ul>
          <li>
            <h3>Web 2.0</h3>
            <p>
              Web pages were initially conceived as structured documents based
              upon HTML. They could include images, video, and other content,
              although the use of media was initially relatively limited and the
              content was mainly static. By the mid-2000s, new approaches to
              sharing and exchanging content, such as blogs and RSS, rapidly
              gained acceptance on the Web. The video-sharing website YouTube
              launched the concept of user-generated content. As new
              technologies made it easier to create websites that behaved
              dynamically, the Web attained greater ease of use and gained a
              sense of interactivity which ushered in a period of rapid
              popularization. This new era also brought into existence social
              networking websites, such as Friendster, MySpace, Facebook, and
              Twitter, and photo- and video-sharing websites such as Flickr and,
              later, Instagram which gained users rapidly and became a central
              part of youth culture. Wikipedia's user-edited content quickly
              displaced the professionally-written Microsoft Encarta. The
              popularity of these sites, combined with developments in the
              technology that enabled them, and the increasing availability and
              affordability of high-speed connections made video content far
              more common on all kinds of websites. This new media-rich model
              for information exchange, featuring user-generated and user-edited
              websites, was dubbed Web 2.0, a term coined in 1999 by Darcy
              DiNucci and popularized in 2004 at the Web 2.0 Conference. The Web
              2.0 boom drew investment from companies worldwide and saw many new
              service-oriented startups catering to a newly "democratized" Web.
              In spite of the success of Web 2.0 applications, the W3C forged
              ahead with their plan to replace HTML with XHTML and represent all
              data in XML. In 2004, representatives from Mozilla, Opera, and
              Apple formed an opposing group, the Web Hypertext Application
              Technology Working Group (WHATWG), dedicated to improving HTML
              while maintaining backward compatibility. For the next several
              years, websites did not transition their content to XHTML; browser
              vendors did not adopt XHTML2; and developers eschewed XML in favor
              of JSON. By 2007, the W3C conceded and announced they were
              restarting work on HTML and in 2009, they officially abandoned
              XHTML. In 2019, the W3C ceded control of the HTML specification,
              now called the HTML Living Standard, to WHATWG.
            </p>
          </li>
          <li>
            <h3>Mobile Web</h3>
            <p>
              Main Article: <a href="mobile_history.html">Mobile History</a>
            </p>
            <br />
            <p>
              Early attempts to allow wireless devices to access the Web used
              simplified formats such as i-mode and WAP. Apple introduced the
              first smartphone in 2007 with a full-featured browser. Other
              companies followed suit and in 2011, smartphone sales overtook
              PCs. Since 2016, most visitors access websites with mobile devices
              which led to the adoption of responsive web design. Apple,
              Mozilla, and Google have taken different approaches to integrating
              smartphones with modern web apps. Apple initially promoted web
              apps for the iPhone, but then encouraged developers to make native
              apps. Mozilla announced Web APIs in 2011 to allow webapps to
              access hardware features such as audio, camera or GPS. Frameworks
              such as Cordova and Ionic allow developers to build hybrid apps.
              Mozilla released a mobile OS designed to run web apps in 2012, but
              discontinued it in 2015. Google announced specifications for
              Accelerated Mobile Pages (AMP), and progressive web applications
              (PWA) in 2015. AMPs use a combination of HTML, JavaScript, and Web
              Components to optimize web pages for mobile devices; and PWAs are
              web pages that, with a combination of web workers and manifest
              files, can be saved to a mobile device and opened like a native
              app.
            </p>
          </li>
          <li>
            <h3>Web 3.0 and Web3</h3>
            <p>
              The extension of the Web to facilitate data exchange was explored
              as an approach to create a Semantic Web (sometimes called Web
              3.0). This involved using machine-readable information and
              interoperability standards to enable context-understanding
              programs to intelligently select information for users. Continued
              extension of the Web has focused on connecting devices to the
              Internet, coined Intelligent Device Management. As Internet
              connectivity becomes ubiquitous, manufacturers have started to
              leverage the expanded computing power of their devices to enhance
              their usability and capability. Through Internet connectivity,
              manufacturers are now able to interact with the devices they have
              sold and shipped to their customers, and customers are able to
              interact with the manufacturer (and other providers) to access a
              lot of new content. This phenomenon has led to the rise of the
              Internet of Things (IoT), where modern devices are connected
              through sensors, software, and other technologies that exchange
              information with other devices and systems on the Internet. This
              creates an environment where data can be collected and analyzed
              instantly, providing better insights and improving the
              decision-making process. Additionally, the integration of AI with
              IoT devices continues to improve their capabilities, allowing them
              to predict customer needs and perform tasks, increasing efficiency
              and user satisfaction. Web3 (sometimes also referred to as Web
              3.0) is an idea for a decentralized Web based on public
              blockchains, smart contracts, digital tokens and digital wallets.
            </p>
          </li>
        </ul>
      </section>
      <h2>Web Levels</h2>
      <ul>
        <li>
          <h3>Surface Web</h3>
          <p>
            The Surface Web (also called the Visible Web, Indexed Web, Indexable
            Web or Lightnet) is the portion of the World Wide Web that is
            readily available to the general public and searchable with standard
            web search engines. It is the opposite of the deep web, the part of
            the web not indexed by a web search engine. The Surface Web only
            consists of 10 percent of the information that is on the internet.
            The Surface Web is made with a collection of public web pages on a
            server accessible by any search engine. According to one source, as
            of June 14, 2015, Google's Index of the Surface Web contains about
            14.8 billion pages
          </p>
        </li>
        <li>
          <h3>Dark Web</h3>
          <p>
            The dark web is the World Wide Web content that exists on darknets:
            overlay networks that use the Internet but require specific
            software, configurations, or authorization to access. Through the
            dark web, private computer networks can communicate and conduct
            business anonymously without divulging identifying information, such
            as a user's location. The dark web forms a small part of the deep
            web, the part of the web not indexed by web search engines, although
            sometimes the term deep web is mistakenly used to refer specifically
            to the dark web
          </p>
          <p>
            The darknets which constitute the dark web include small,
            friend-to-friend networks, as well as large, popular networks such
            as Tor, Freenet, I2P, and Riffle operated by public organizations
            and individuals. Users of the dark web refer to the regular web as
            Clearnet due to its unencrypted nature. The Tor dark web or
            onionland uses the traffic anonymization technique of onion routing
            under the network's top-level domain suffix .onion.
          </p>
          <p>
            The dark web has often been confused with the deep web, the parts of
            the web not indexed (searchable) by search engines. The term dark
            web first emerged in 2009; however, it is unknown when the actual
            dark web first emerged. Many internet users only use the surface
            web, data that can be accessed by a typical web browser. The dark
            web forms a small part of the deep web, but requires custom software
            in order to access its content. This confusion dates back to at
            least 2009. Since then, especially in reporting on Silk Road, the
            two terms have often been conflated, despite recommendations that
            they should be distinguished
          </p>
          <p>
            The dark web, also known as darknet websites, are accessible only
            through networks such as Tor ("The Onion Routing" project) that are
            created specifically for the dark web. Tor browser and
            Tor-accessible sites are widely used among the darknet users and can
            be identified by the domain ".onion". Tor browsers create encrypted
            entry points and pathways for the user, allowing their dark web
            searches and actions to be anonymous.
          </p>
        </li>
        <li>
          <h4>Tor Network</h4>
          <p>
            Tor is a free overlay network for enabling anonymous communication.
            Built on free and open-source software and more than seven thousand
            volunteer-operated relays worldwide, users can have their Internet
            traffic routed via a random path through the network.
          </p>
          <p>
            Using Tor makes it more difficult to trace a user's Internet
            activity by preventing any single point on the Internet (other than
            the user's device) from being able to view both where traffic
            originated from and where it is ultimately going to at the same
            time. This conceals a user's location and usage from anyone
            performing network surveillance or traffic analysis from any such
            point, protecting the user's freedom and ability to communicate
            confidentially.
          </p>
          <p>
            The core principle of Tor, known as onion routing, was developed in
            the mid-1990s by United States Naval Research Laboratory employees,
            mathematician Paul Syverson, and computer scientists Michael G. Reed
            and David Goldschlag, to protect American intelligence
            communications online. Onion routing is implemented by means of
            encryption in the application layer of the communication protocol
            stack, nested like the layers of an onion. The alpha version of Tor,
            developed by Syverson and computer scientists Roger Dingledine and
            Nick Mathewson and then called The Onion Routing project (which was
            later given the acronym "Tor"), was launched on 20 September 2002.
            The first public release occurred a year later.
          </p>
          <p></p>
        </li>
        <li>
          <h3>Deep Web</h3>
          <p>
            The deep web, invisible web, or hidden web are parts of the World
            Wide Web whose contents are not indexed by standard web
            search-engine programs. This is in contrast to the "surface web",
            which is accessible to anyone using the Internet
          </p>
          <p>
            Deep web sites can be accessed by a direct URL or IP address, but
            may require entering a password or other security information to
            access actual content. Uses of deep web sites include web mail,
            online banking, cloud storage, restricted-access social-media pages
            and profiles, and web forums that require registration for viewing
            content. It also includes paywalled services such as video on demand
            and some online magazines and newspapers.
          </p>
        </li>
      </ul>
      <section>
        <h2>Future Expectation</h2>
        <ul>
          <li>
            <h3>Beyond Web 3.0</h3>
            <p>
              The next generation of the Web is often termed Web 4.0, but its
              definition is not clear. According to some sources, it is a Web
              that involves artificial intelligence, the internet of things,
              pervasive computing, ubiquitous computing and the Web of Things
              among other concepts. According to the European Union, Web 4.0 is
              "the expected fourth generation of the World Wide Web. Using
              advanced artificial and ambient intelligence, the internet of
              things, trusted blockchain transactions, virtual worlds and XR
              capabilities, digital and real objects and environments are fully
              integrated and communicate with each other, enabling truly
              intuitive, immersive experiences, seamlessly blending the physical
              and digital worlds"
            </p>
          </li>
        </ul>
      </section>
    </main>
  </body>
</html>
